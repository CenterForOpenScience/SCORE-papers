Investigating the replicability of the social and behavioral sciences


Authors


Abstract


We conducted replications of {n_claims} claims from {n_papers} papers published between 2009 and 2018 in 62 journals in the social and behavioral sciences. Replications were high-powered, used original materials when relevant and available, and were peer reviewed in advance through a standardized internal protocol. On average, replication effect sizes were {p_effect_size_ratio_v_orig}% smaller than original effect sizes and {p_findings_stat_sig_and_in_direct}% of replication findings were statistically significant and in the same direction as the original finding. Replication effect sizes were smaller than original effect sizes in business ({p_effect_size_smaller_v_orig_business}%, n={n_effect_size_smaller_v_orig_business}), economics and finance ({p_effect_size_smaller_v_orig_econ}%, n={n_effect_size_smaller_v_orig_econ}), education ({p_effect_size_smaller_v_orig_edu}%, n={n_effect_size_smaller_v_orig_edu}), political science ({p_effect_size_smaller_v_orig_polisci}%, n={n_effect_size_smaller_v_orig_polisci}), psychology and health ({p_effect_size_smaller_v_orig_psych}%, n={n_effect_size_smaller_v_orig_psych}), and sociology and criminology ({p_effect_size_smaller_v_orig_soc}%, n={n_effect_size_smaller_v_orig_soc}). Year of publication was [positively, negatively, weakly/not] associated with relative effect size between replication and original studies. [interpretation] [conclusion] 








Keywords: replication, credibility, reliability, validity, economics, political science, psychology, marketing, sociology, finance, management, public administration, organizational behavior, education, criminology, health research
________________
Evidence being replicable is an important contributor to the usefulness of knowledge claims. Replication refers to testing the same research question as a prior investigation with different data.1,2 Establishing replicability of evidence suggests that a regularity in nature has been uncovered. Knowledge production can then advance further by investigating the validity of the methods for generating the evidence, and interrogating the relationship between the evidence and the knowledge claims. Failure to establish replicability is a barrier to knowledge production because it remains unknown whether there is a regularity to describe, predict, or explain.


Recent investigations have provided evidence that the published research literature may not be as replicable as believed or desired. Across several systematic replication studies in the social and behavioral sciences, approximately half of well-powered replication studies provided statistically significant evidence in the same direction as an original finding.3–11 Moreover, across these investigations, observed effect sizes in replication studies were about half as large as effect sizes in original studies on average10, and as much as 85% weaker in a sample of preclinical cancer biology replications.12 


Popular explanations for the observed low replication rates include: the existence of selective reporting, particularly favoring the publication of positive evidence for claims and ignoring of negative evidence; the use of small samples, unreliable measures, and low powered research designs, coupled with a statistical threshold (p < .05) that serves as a publication filter; the use of questionable research practices that inflate the likelihood of obtaining positive outcomes at the cost of their credibility; low rigor and quality control in research design and measurement; and, failing to distinguish between findings that are the outcomes of data-dependent versus data-independent decision-making as the former are more vulnerable to mistaking noise as signal.10,13–20 Collectively, these practices are perceived to be symptoms of a dysfunctional reward system in publishing that favors positive, novel, tidy findings, and a social system lacking in transparency of what and how research is conducted to increase accountability and facilitate evaluation and self-correction.21–23


Discussion of potential causes presupposes that the evidence of relatively low replicability is itself replicable. Moreover, to date, most of the systematic evidence comes from a few disciplines, but the underlying concern is that the factors that could undermine replicability may exist across many research domains. Here, we report a replication of systematic replication studies. This replication study, part of the SCORE program funded by DARPA, conducted replications of claims from a sample of papers from a diverse sample of {n_journals} [a]journals across the social and behavioral sciences (Table 1). Those journals represented subfields of the social and behavioral sciences that are aggregated for expository purposes into six domains: {n_journals_business} journals in Business (including Organizational Behavior, Management, and Marketing), {n_journals_econ} journals in Economics (including Finance), {n_journals_edu} journals in Education, {n_journals_polisci} journals in Political Science, {n_journals_psych} journals in Psychology (including Health), and {n_journals_soc} journals in Sociology (including Criminology). (See SI for outcomes separately by subfield.)


Findings for replication were identified with a systematic selection process to reduce selection bias and increase generalizability of the findings to quantitative social and behavioral research. The project started[b] with a sample of {n_papers_initial_sample} papers selected by a stratified random sampling from a larger set of papers to ensure representativeness across the 62 journals and publication dates from 2009 to 2018. From that pool, {} papers were randomly selected with a similar stratified random sampling process to maintain representativeness as the papers eligible for conducting replication studies (see Figure X).


Figure X
{figure_descriptive_flow[c][d]}


Eligible papers were matched with research teams with relevant expertise to design and conduct the replication study. Here, random sampling is lost because selection is based on feasibility, available resources, and available expertise. Of {n_papers_eligible} papers made eligible for selection for replication, {n_papers_matched_with_researchers} papers were matched with a research team. 


Whenever possible, original methods and materials were collected from the original authors and adapted for the replication study. Replication teams prepared the research design including the methodology and analysis plan and put those through a peer review process that was managed by an independent editor and included independent reviewers plus at least one author of the original study if they agreed to provide review. Replication designs could involve either the collection of new data or finding independent, existing data that was not used for the original research. The former was more frequent for experimental methods such as common practice in psychology and marketing; the latter was more frequent for observational methods such as common practice in economics and political science. Approved designs and analysis plans were preregistered on the OSF prior to conducting the research. Of {n_papers_matched_with_researchers} papers matched with a research team, {} replication study designs were reviewed and preregistered ({}%). For the purposes of this project, preregistration was the milestone defining that the replication had started. {} of {} ({}%) of started replications were completed (see SI for information about attrition of replications).


In most,[e] but not all, cases, a single claim was identified in a single paper and subjected to a single replication attempt with independent data. Figure X visualizes the selection process and highlights the variations from the modal case. Of the {n_papers} papers eligible for replication, {n_papers_single_claim} isolated a single claim for replication and {n_papers_multi_claim} extracted additional claims that could be replicated. In {} cases, multiple replications were conducted of a single claim using the same protocol, akin to “many labs” studies (citation). And, in {} cases, multiple replications were conducted of a single claim using distinct protocols. For both of these cases, the primary reporting aggregates evidence across multiple replications of a single claim (see the SI for outcomes of multiple replications of single claims). Finally, there were {n_repli_hybrid} replications that partially met the criterion of using new data. These “hybrid” replications added new data to data that had been used in the original research. We included these because features of their methodology or research question made them relevant tests of replicability, such as adding additional years to testing a longitudinal hypothesis (see SI for outcomes of the hybrid replications). 


Completed replication reports were reviewed by an internal process by researchers not involved in the replication study for quality control, and original authors were invited to review the report and post a commentary if they wished on the OSF with the final report. All data, materials, and code were archived on the OSF and made possible to the maximum extent allowed without violating privacy of participants or intellectual property licenses for any original materials. A total of {n_repli_total} replications were conducted and, following aggregation evidence for multiple replications of a single claim there were {n_claims} replications of unique claims from {n_papers} papers.


Results[f]


Summary of replications completed in comparison with the sampling frame[g]


The sampling strategy reduced but did not eliminate selection effects that could impact the generalizability of these findings. The early stages of the sampling process mostly avoided unknown selection biases by establishing a sampling frame and systematically selecting papers and claims that met the inclusion criteria. The later stages of the sampling process introduced more opportunities for selection bias to shape the sample as decisions were made about the feasibility of conducting a replication, the availability of sufficient resources, and matching replications with research teams with the expertise and instrumentation to conduct the research. 


Table X[h] illustrates how the proportion of replicated papers and claims by discipline differed from the original stratified random sample. [descriptive insights highlighting biggest differences and areas of similarity]


Table Y[i] illustrates how the proportion of replicated papers and claims by year differed from the original stratified random sample. [description of what is observed.]


These results highlight potential biases in generalizability introduced in the selection and conducting of replication studies from the sampling frame on the dimensions that we deliberately attempted to maintain as much representativeness as possible. There are other features of the sample for generalizability considerations such as the inclusion criteria of requiring a statistical inference that could be the basis of a replication study. 


Evaluating replication effect against null hypothesis of no effect


A common method for evaluating whether evidence is obtained for an original claim is to conduct a statistical test against the null hypothesis and assess whether the observed test statistic exceeds a statistical threshold (usually alpha = .05) to reject the null hypothesis. The same method is often used for assessing whether a replication study finds statistically significant evidence in the same direction of the original study. This approach has some advantages such as simplicity, parallelism to how original conclusions are drawn, and applicability across a variety of statistical models. It also has some drawbacks such as dichotomous assessment and failure to incorporate indicators of precision of estimation. Nevertheless, it is a popular method and this outcome was used in the project by human and machine teams as the criterion for prediction. [j]


According to the inclusion criteria, original claims had to be a finding that achieved statistical significance according to the paper’s stated criterion: {n_repli_stat_sig_claims} of {n_claims} original claims ({}%) were supported by a statistically significant effec[k]t. Across the {} of {} replication outcomes for which this criterion could be applied,[l] we observed {} ({}%) replications showing a statistically significant effect in the same direction as the original finding and {} ({}%) replications showing a null effect or a significant effect in the opposite direction. 


{} of the significance tests were directionless, and {} could be in the same or opposite direction of the original finding. For the latter, we applied 2-tailed tests and observed that {} of {} were statistically significant in the opposite direction of the original finding.[m] Based on the power of the replication studies, if all replications were statistically consistent with original findings in reality, then we would expect {} of {} ({}%) to be statistically significant and in the same direction, which is greater than the observed rate of {}%. [dichotomous assessment[n]]


Evaluating replication effect size against original effect size


Another common method for evaluating whether replication evidence is similar to original evidence for a claim is to compare the observed effect sizes. This has advantages such as offering a continuous representation of the effect magnitudes and concordance with the aim of synthesizing research evidence across many studies to draw more precise and general conclusions about research claims. It also has disadvantages in systematic replication projects like this due to the lack of standardization of effect size metrics across statistical methods. There are some methods of converting effects in raw units to standardized units that can be applied across different models with few assumptions. But, there are others that require many assumptions about the underlying data and comparability of the original and replication research designs. And, there are others for which there is no possibility of establishing a common metric. For the purposes of characterizing the results of this systematic replication effort, we sought effect size metrics that would be as standardized across claims as possible, and as customized as was necessary. Here, we report the aggregate effect sizes in the simplest available format, and in the supplementary information we provide summaries of alternative effect size calculations that require fewer assumptions but are only applicable to smaller subsets of the claims. The qualitative interpretation of the effect size comparisons does not change between the different approaches, but there is some variation, especially for some individual studies for which the assumptions make a meaningful difference in the estimated effect sizes (see SI for details)[o][p].


{figure_effect_size_comparison}[q]


Figure X represents the replication effect sizes (y-axis) plotted against the original effect sizes (x-axis) separately for those computed using the X statistic (left panel; n = {}) and those using the Y statistic (right panel; n = {}). 


Among the {} claims for which a [ES] could be calculated for the original and replication study, the median original effect size was {} (SD = {}) and the median replication effect size was {} (SD = {}), or {}% smaller. And, among the {} claims for which a [ES] could be calculated for the original and replication study, the median original effect size was {} (SD = {}) and the median replication effect size was {} (SD = {}), or {}% smaller. Across effect size estimates, replication effects were {}% smaller than original effects on average. [if there are substantial qualifiers of the general conclusion, describe them.][general conclusion]


Meta-analytic evidence from combining original and replication effect sizes


Combining the evidence from original and replication studies uses all available evidence to assess research claims. Meta-analytic summaries have advantages such as being the long-run preferred method for accumulating total evidence for a claim, and disadvantages such as ignoring factors that might influence the quality or weighting of evidence other than estimates of precision. For example, if original findings were produced in the context from the introduction with questionable research practices that inflate false positives and observed effect sizes, then that exaggerated evidence is included in the meta-analytic estimate.


[outcomes][qualifications][conclusion]


Other replication success criteria


[r]


Original and replication effects by discipline


We included research claims from across the social-behavioral sciences including psychology where there have been several systematic replication studies, economics where there is some evidence from systematic replication efforts, and business, education, political science, and sociology where there is less evidence outside of replications of individual studies. 


Table X summarizes outcomes separately by discipline for the statistical significance and effect size comparisons of original and replication studies. [Summarize on the various dimensions]


[Is there a productive figure to make that would show the effect size comparison? If there are two panels for the main display for different effect sizes, 12 panels is getting to the limit of comprehension unless we can provide some simplifying visual heuristics such as increasing the emphasis of the bottom line comparator.]


Though the total number of replications is relatively large, there is not sufficient evidence to draw strong conclusions about differences in replicability rates between disciplines in this study. Rather, what is indicated by these findings is that there is no indication that any subfield of the social-behavioral sciences is an exception to the accumulating evidence that achieving high replicability is challenging. [connect to evidence, conclusion]


Original and replication effects by year of original publication


Given the stratified sampling strategy, we can also examine whether there is observable variation in replication success based on the year of the original publication. Observed variation might stimulate hypothesizing of possible reasons that could be investigated in follow-up research. Table/Figure X summarizes the correlations between year of publication and replication success on several metrics. [A correlational analysis here, or are there some creative visualizations to provide?]


Original and replication effects by new data or secondary data replication


Replications could be completed by generating new data or by finding independent data that could examine the same question. It is possible that replication success could vary by these methods. For example, one hypothesis could be that greater control over the experimental paradigm would lead to greater replication success with new data than having to rely on what is available in existing data to conduct a fair test of an original claim. 


Notably, use of new data or secondary data is confounded to some extent by discipline with a larger proportion of secondary data replications in field1 ({} of {} replications) and field2 ({} of {}), and a smaller proportion of secondary data replications in field3 ({} of {}), field4 ({} of {}), field5 ({} of {}), and field6 ({} of {}). Table X presents…


Exploratory analysis of potential correlates of replication outcomes


Discussion


About half of the findings from a sample of social and behavioral science papers published between 2009 and 2018 replicated successfully across a variety of criteria. Effect sizes of replication studies were about {}% smaller than effect sizes of original studies. The likelihood of successful replication varied little across time and somewhat across disciplines, and no field showed replication success rates above {}% on the primary criteria. These findings, from the largest systematic replication effort ever conducted, are consistent with the cumulative evidence across systematic replication efforts in the social and behavioral sciences (citations), and from other fields (citations). The underwhelming replicability of published findings is, itself, replicable.


Simultaneously, the findings do not justify concluding that the published literature is not credible. Approximately half of published findings did replicate successfully in terms of showing statistically significant evidence in the same direction as the original result. And, given the observed power in relation to the original findings, the maximum anticipated replication rate on this criteria was {}%. Moreover, about {}% of the findings were within 0.x standard deviations of the original finding suggesting that a sizable minority of findings had similar effect sizes between original and replication studies. A bleak conclusion that “nothing replicates” is false. Rather, the published evidence base provides exaggerated evidence, on average, and it would be useful to know which evidence. 


Understanding replication


Failure to replicate does not mean the claim was wrong. A single failure to replicate does not justify the conclusion that the original result was wrong. Even if the replication research was perfectly designed, the effect could be missed or underestimated because of sampling error, a false negative. Even if the replication research appeared to be testing the same research question, there could be differences in the methodology, sample, or context that are unrecognized moderators of the likelihood of observing the findings. And, even if the replication researchers were diligent in conducting the research, there could be unrecognized errors or flaws in the implementation that interfered with observing the finding.


The studies included in this replication effort attempted to minimize these reasons for failing to replicate by using research designs that were well-powered[s][t] to detect the original effect size ({}% of replications had >{}% power), obtaining and adapting original materials whenever possible to design an effective replication, conducting peer review in advance of all replication studies -- including original authors in the peer review whenever possible ({}% of replication studies had an original author serve as a reviewer), preregistering all replication studies, and maximizing accountability for replication authors by committing that all materials and data available would be publicly accessible for post-publication review of the methodology. These efforts justify increased confidence in the rigor of the replication studies, but do not justify treating the observed outcomes as sacrosanct. Productive follow-up research may provide counterevidence to some or many of the replication findings. And, in the long run, the cumulative evidence across many investigations will provide the strongest basis for drawing conclusions about research claims.


        Successful replication does not mean the claim was right. A single successful replication does not justify the conclusion that the original result was correct. The effect could be observed because of sampling error, a false positive. Flaws in the research design could coincidentally cause a similar outcome but for a different reason. And, most critically, the replicability of an effect is not the same as the validity of the evidence for drawing the conclusion. Original and replication studies may share confounds, faulty measures, or other design weaknesses that produce positive results for different reasons than the interpretation of the evidence. 


        The optimal replicability rate is not known. The maximum expected replicability rate on the statistical significance criterion in this research was {}%. That would have been evidence of “perfect replicability” taking into account expected sampling error given the sample sizes of the replication studies. But, it is not clear that achieving that rate of replicability of original findings should be the objective of the research enterprise at all stages of research. In discovery oriented contexts, it is understood that reaching into the unknown will produce many false leads. Whereas, in translation of research claims to policy and practice, it would be quite unproductive to apply findings from a literature with weak replicability. 


The problem to solve is not unreplicability per se; the problem to solve is overconfidence. “Published and true are not synonyms” (Nosek et al., 2012). Improving calibration of claims with evidence will expose that most published claims are more uncertain than is presently appreciated. For many published findings, it is uncertain whether they will replicate at all, whether they are robust to seemingly minor variations in the research context, whether they are generalizable across the presumed relevant areas of application, and whether they are valid interpretations of the actual evidence. Recognition of that uncertainty will reduce overconfidence and increase the valuation of replication and other forms of verification research to confront present understanding by testing replicability, robustness, generalizability, and validity of research findings (Nosek & Errington, 2021). 


Constraints on generalizability


The sampling frame was a biased representation of the social-behavioral sciences because of the requirement of the claim to be supported by a statistical inference. This was practically sensible for the purposes of the program, but means that this evidence does not speak to the credibility and replicability of qualitative research in the social and behavioral sciences. There is a productive and active debate on the meaning and potential for replication and credibility assessments in qualitative research, but the present findings do not offer any meaningful insight to that scholarship.


The sampling frame covered a wide range of the social and behavioral sciences by drawing from 62 journals and papers published between 2009 and 2018. However, it is conceivable that the selection of relatively prominent journals led to replicability outcomes that were higher or lower than what would be observed if less prominent journals were selected. Likewise, replicability outcomes might have been higher or lower if sampling had reached further back in history, and replicability might be changing in research published during and after the conducting of this project.


Selection biases were minimized within the sampling frame with stratified random sampling of papers that met the inclusion criteria, but selection biases were introduced in the conducting of replications.  The dominant reason for selection bias was feasibility of conducting a replication. Some findings were based on longitudinal designs, special samples, or expensive methodologies that went beyond the available resources for the project. The implications of selection biases based on feasibility are unknown, and it is not difficult to generate plausible hypotheses that original findings from more resource intensive research would be more, less, or similarly replicable as original findings from less resource intensive research.


The general observation that replication effects are weaker than original studies is observed across multiple systematic replication efforts, each with distinct sampling strategies and selection biases. There is no existing theory or evidence that the recognizable selection biases would lead to over- or under-estimation of replicability. But, these would be productive avenues to examine systematically.


Finally, there is initial evidence of research activity to which these findings do not generalize. Protzko and colleagues (2023) conducted a prospective replication study in which 16 discoveries from four social-behavioral science laboratories were subjected to confirmatory testing and round-robin replication in independent samples by the other laboratories and the original lab. The labs adopted behaviors suggested by methodologists as potential solutions for improving replicability of findings including the original laboratory conducting a preregistered confirmatory test once they believed they had a new discovery, using large samples to make precise estimates, sharing research materials across labs, and preregistering all replication studies. They observed that replication effect sizes were 97% the size of the confirmatory findings, and that the proportion of findings achieving statistical significance was as high as could be expected given the statistical power of the replication studies. This suggests that there are conditions under which weaker evidence from replication studies will not be observed systematically, and a productive avenue for future research will be to identify those conditions.


Implications for the credibility of the social and behavioral sciences


The presumed causes of weak replicability in quantitative research including underpowered studies, selective reporting favoring positive results, and questionable research practices, are prevalent across research domains (citations), but most systematic replication research has occurred in psychology leading to a gap between the presumed breadth of replicability challenges and the evidence base. The present findings addresses that gap and indicates that replicability challenges may be similar across the social and behavioral sciences. The evidence was not sufficient to draw strong conclusions about whether some sub-disciplines are more replicable than others, though there were hints that {}. And, of course, there isn’t presently any evidence to suggest that these challenges are limited to the social and behavioral sciences. In fact, a systematic investigation of preclinical cancer biology findings found even weaker evidence for replicability on some criteria than was observed in this research (citations). The most defensible conclusion based on the present evidence is that replicability is likely to be improvable across the breadth of the social and behavioral sciences. And, it may be that improvements to replicability of findings in sub-disciplines beyond psychology can be accelerated by drawing on the last several years of accumulating evidence and practices for improving rigor and replicability in psychology.


Is the reform movement to improve research credibility making any progress?


During the last decade, attention to credibility challenges and opportunities to improve has been high in the social and behavioral sciences, especially psychology, economics, and political science. The weak evidence for change between 2009 and 2018 suggests that whatever reforms that occurred during that time period was not sufficient to alter replicability. This stands in contrast to evidence for improvement in reproducibility, at least within economics and political science, during the same time period following the introduction of journal policies requiring sharing data and code (citation). There are at least three likely contributors to the stark difference in improvements to reproducibility and replicability during this time frame. 


First, the behaviors believed to be necessary for establishing reproducibility occur at the end of the research lifecycle. Good documentation and sharing of data and code should be sufficient to achieve reproducibility of published findings. On the other hand, some of the behaviors believed to contribute to establishing replicability occur earlier in the research lifecycle, such as preregistration. As a consequence, it will take longer for research that incorporates improved practices across the research lifecycle to appear in the published literature. For example, Figure X illustrates the growth in papers mentioning preregistration over time for the disciplines represented in this project. Papers mentioning preregistration increases by 66% (Education) to 1279% (Psychology) between 2009 and 2018, but still represented only a very small fraction of papers in 2018, from a low of 0.16% in business to a high of 1.13% in psychology. And, of {} papers assessed in the present sample, just {} ({}%) reported having preregistered at least one of the reported studies. Figure X also illustrates continuing growth since 2018, particularly in psychology (increase of 7303% in 2023 over 2009; and 6.47% of all papers), suggesting that it will be valuable to examine whether replicability rates are changing in the future based on adoption of preregistration and other rigor-enhancing reforms.


Figure X[u]. Percentage of papers mentioning preregistration, pre-analysis plans, or synonyms by discipline from 2009 to 2023. Full range visible in left panel; Range scales to 1% in right panel.
  
  



Second, the behaviors that should be sufficient for establishing reproducibility -- sharing data and code -- have been incentivized or made part of required policy by a larger number of stakeholders during the past decade than the additional behaviors that could improve replicability, such as higher powered research, preregistration, sharing materials, and improved theory for generating robust hypotheses. For example, Table Y illustrates the proportion of journals in sub disciplines of social and behavioral sciences that have adopted policies for data sharing, code sharing, materials sharing, and preregistration[v]. Moreover, [citation] provided suggestive evidence that the policy requirement by several economics journals to demonstrate independent reproducibility along with sharing data and code further enhanced achieving reproducibility as compared with journals and fields, like political science, that just require sharing data and code and not demonstrating reproducibility. 


And, third, achieving replicability is harder than achieving reproducibility. There are main reasons that a finding will fail to replicate. And, in the ordinary course of discovery and description through prediction and explanation, failure to replicate will be common. Initial observation of a finding might be a false positive, or might be observation of a true effect but one that occurs under much more limited conditions than initially expected or for different reasons that initially understood.  Productive maturation of an area of research is characterized by identification of the conditions that are necessary or sufficient to observe the finding, and the maturation of a theoretical understanding of what those conditions mean.


Nevertheless, there is at least preliminary evidence that improving replicability is achievable. With the adoption of the rigor-enhancing behaviors across the research lifecycle, Protzko and colleagues (2023) observed replicability evidence highly aligned with original confirmatory findings. The next step, like for all research, is to clarify the conditions under which such high replicability is replicable.


Conclusion
[w]


Our evidence is consistent with prior systematic replication studies, and holds across the included disciplines. We conclude that the evidence for published knowledge claims is stronger in original studies than replication studies on average, and that this likely generalizes across a substantial portion of quantitative research in the social and behavioral sciences. This regularity is worthy of additional investigation to predict, explain, and address to enhance the credibility of research. Considering replicability is an issue for all areas of research. Making progress on the reliability of evidence will open pathways for accelerating progress on developing theory to explain that evidence[x].




Method
This systematic replication effort was part of the SCORE program funded by DARPA to generate and evaluate automated measures of confidence in research claims.24 Replications provide test data to evaluate the accuracy of human and machine predictions of replicability of claims. Evidence for reproducibility (same analysis, same data) and robustness (different analysis, same data) were also gathered as part of the program. Relations among credibility assessments are reported in {}, and evidence for algorithms accuracy predicting replication success is reported in {}. A full report of the SCORE methodology is accessible through a Master Supplement (CITE), and a collection of supplements for specific components of the program, including one for the replication studies (CITE). Also, all data, materials, code, and other outputs from the program are organized and publicly accessible for evaluation and re-use in an OSF Collection (CITE). This methods section summarizes critical features of sampling, conducting the replication studies, aggregating the data across replications, and assessment of replication success. 


Sampling frame and selection of claims for replication
There is no definable population of social and behavioral science research because the boundaries between research domains are fuzzy. Nevertheless, we sought a systematic sampling strategy that would maximize representatives for a broad portion of the social and behavioral sciences. We defined the sampling frame as articles published in 62 journals between 2009 and 2018. From those journals, we created a stratified random sample of {},{} papers that was available for the whole program. From those papers, we created a stratified random sample of {} papers from which a single claim was extracted from each paper. A claim eligible for replication was a statistical inference associated with a conceptual claim made in the paper’s abstract (see Supplement S2 for details). From those papers, we created a stratified random sample of 600 papers and associated claims that were eligible for selection to be replicated. From those papers, we defined a sample of 200 papers that was enhanced further by extracting all relevant claims from the paper. Details of these sampling procedures are in the “Sampling frame” section of Supplement S1.


Claims from the 600 papers in the Evidence Set were eligible for selection to be replicated. We used existing social networks and social media to invite social-behavioral researchers to join the SCORE project and indicate interest in conducting replication studies. [summary of how researchers were matched with replication claims]


Replication procedures


Data aggregation


Conversions to get data on common metrics
Meta-analytic combinations of original and replication data


Evaluating replication outcomes
The goal is to assess the replicability of the (single) main finding of many original studies. We did this using statistical results of pairs of original and corresponding replication studies. The unit of analysis is the main finding of the original study, and for each unit, there can be no, one, or multiple statistical results, depending on how many replication studies were carried out to replicate that main finding. This approach yields five measures, three dichotomous and two continuous, both on the evidence against the null and effect size. As the continuous measures can be expected to have outliers (e.g., because a huge sample size yields a very large z-value in the replication study or because EORG was very close to 0), we planned to use Spearman-rank order correlations rather than Pearson correlations involving these two continuous measures.


Evidence against the null or the existence of the effect. We define three measures, all relying on p-values corresponding to the statistical test of the original main finding and of the main finding that is replicated in one or more replication studies.


The first measure is solely based on the conclusion of the statistical test, using p < .05, and assumes the original main finding was statistically significant. The outcome of this measure is calculated as ‘1’ if the replication study/studies yielded the same conclusion (i.e., statistically significant and, if relevant, also in the same direction) on the main finding as the original study, and ‘0’ otherwise. In case of multiple replication studies the statistical significance of the replication studies is based on the statistical significance of the effect size obtained with fixed-effect meta-analysis (i.e., weighted average using inverse variance weighting; if the standard error of the effect of each replication study is reported, or simply using the unweighted average in case standard errors are unknown).


The second measure focuses on replicability in the sense of obtaining the same conclusion in the original and replication study/studies, also using p < .05. The same conclusion is coded as ‘1’, another conclusion as ‘0’. Here too, a fixed-effect meta-analysis is applied in case of multiple replication studies.


The third measure provides a comparison of continuous evidence against the null hypothesis of no effect. First, all p-values are converted to z-values, like in Stouffer’s method of meta-analysis. Second, we compute zREP – zORG, with zREP equal to the average of all z-values converted from all p-values of the replication study/studies of the main finding.


One drawback of the three considered measures is that they combine sample size and effect size. That is, in case of a non-zero true effect, the probability of a statistically significant outcome (also known as the statistical power of the test) increases, and the p-value, on average, decreases as the sample size increases. Consequently, if the replication study examines a smaller true effect size than the original study, the statistical evidence against the null may still likely be stronger in the replication than in the original study if the sample size is (much) larger in the replication study than in the original study.


Size of the effect. Unfortunately, effects corresponding to main findings from different studies are assessed on very different effect size scales (e.g., Cohen’s d, odds ratio, regression coefficient in a multilevel analysis, etc.) that cannot be meaningfully converted to one and the same scale. Fortunately, an original main finding and its corresponding replication findings are assessed on the same scale. Hence, we introduce two measures based on a comparison of the original and replication study’s effect sizes. Again a dichotomous and a continuous measure are defined.


The dichotomous measure is based on the difference between the average of the effect across the replication studies and the effect of the original study on the main finding, EREP – EORG. If this difference is negative, the measure equals 1 for that finding, otherwise the measure takes on the value of 0. In case of multiple replication studies, EREP is calculated using fixed-effect meta-analysis (i.e., weighted average using inverse variance weighting) if the standard error of the effect of each replication study is reported, or simply using the unweighted average in case standard errors are unknown.


The continuous measure tries to convert statistical results to the same scale by computing the ratio (EREP – EORG)/EORG, where E is computed as before. The values of this measure are to be interpreted as follows:
< -1:                effect of the replication study in the other direction
(-1, 0):                effect smaller in the replication study, but in the same direction
0:                effect is the same in both studies
> 0:                effect is larger in the replication study, and in the same direction


Although measures such as this ratio are often used, for instance, to compute biases of statistics in simulation studies, these measures have (at least) one disadvantage; the variability of the ratio decreases in the size of the effect of the original study, with possibly very high variability in case of an effect close to 0 in the original study.


Meta-analytic evidence. 




Statistical procedures[y]




________________


References


1.        Nosek, B. A. & Errington, T. M. What is replication? PLOS Biol. 18, e3000691 (2020).
________________
2.        National Academies of Sciences, E. Reproducibility and Replicability in Science. (2019). doi:10.17226/25303.
3.        Open Science Collaboration. Estimating the reproducibility of psychological science. Science 349, aac4716–aac4716 (2015).
4.        Klein, R. A. et al. Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Adv. Methods Pract. Psychol. Sci. 1, 443–490 (2018).
5.        Klein, R. A. et al. Investigating Variation in Replicability: A “Many Labs” Replication Project. Soc. Psychol. 45, 142–152 (2014).
6.        Ebersole, C. R. et al. Many Labs 5: Testing pre-data collection peer review as an intervention to increase replicability (results-blind manuscript). (2019). doi:10.31234/osf.io/sxfm2.
7.        Ebersole, C. R. et al. Many Labs 3: Evaluating participant pool quality across the academic semester via replication. J. Exp. Soc. Psychol. 67, 68–82 (2016).
8.        Camerer, C. F. et al. Evaluating replicability of laboratory experiments in economics. Science 351, 1433–1436 (2016).
9.        Camerer, C. F. et al. Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015. Nat. Hum. Behav. 2, 637–644 (2018).
10.        Nosek, B. A. et al. Replicability, Robustness, and Reproducibility in Psychological Science. Annu. Rev. Psychol. (2021).
11.        Cova, F. et al. Estimating the Reproducibility of Experimental Philosophy. Rev. Philos. Psychol. 12, (2018).
12.        Errington, T. M. et al. Investigating the replicability of preclinical cancer biology. eLife 10, e71601 (2021).
13.        Munafò, M. R. et al. A manifesto for reproducible science. Nat. Hum. Behav. 1, 0021 (2017).
14.        Ioannidis, J. P. A. Why Most Published Research Findings Are False. PLoS Med. 2, e124 (2005).
15.        Button, K. S. et al. Power failure: why small sample size undermines the reliability of neuroscience. Nat. Rev. Neurosci. 14, 365–376 (2013).
16.        Wagenmakers, E.-J., Wetzels, R., Borsboom, D., van der Maas, H. L. & Kievit, R. A. An agenda for purely confirmatory research. Perspect. Psychol. Sci. 7, 632–638 (2012).
17.        Simmons, J. P., Nelson, L. D. & Simonsohn, U. False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychol. Sci. 22, 1359–1366 (2011).
18.        Greenwald, A. G. Consequences of prejudice against the null hypothesis. Psychol. Bull. 82, 1–20 (1975).
19.        Rosenthal, R. The file drawer problem and tolerance for null results. Psychol. Bull. 86, 638–641 (1979).
20.        Nosek, B. A., Ebersole, C. R., DeHaven, A. C. & Mellor, D. T. The preregistration revolution. Proc. Natl. Acad. Sci. 115, 2600–2606 (2018).
21.        Giner-Sorolla, R. Science or Art? How Aesthetic Standards Grease the Way Through the Publication Bottleneck but Undermine Science. Perspect. Psychol. Sci. 7, 562–571 (2012).
22.        Nosek, B. A., Spies, J. R. & Motyl, M. Scientific Utopia: II. Restructuring Incentives and Practices to Promote Truth Over Publishability. Perspect. Psychol. Sci. 7, 615–631 (2012).
23.        Nosek, B. A. et al. Promoting an open research culture. Science 348, 1422–1425 (2015).
24.        Alipourfard, N. et al. Systematizing Confidence in Open Research and Evidence (SCORE). (2021).
________________


NOTES


Concept: We expect to complete replication studies for approximately 190 papers producing 327 replication outcomes from a stratified sample of 62 social-behavioral science journals. The approach expanded on the model of the Reproducibility Project: Psychology that reported 100 replications in psychology in 2015, including a pre-data collection peer review process that was established for the purposes of the program. This paper dramatically expands the evidence about replicability across social and behavioral science disciplines including criminology, economics, education, health, management, marketing, organizational behavior, political science, psychology, public administration, and sociology. Also, given the sampling strategy, we can report on whether replicability rates differ substantially across a decade (2009 to 2018) and between other types of substantive distinctions such as whether replication success varies between new data collections in experimental contexts (as is common in marketing and psychology) versus uses of different datasets in secondary data analysis contexts (as is common in political science and sociology). This paper will be co-authored by hundreds of researchers that organized and contributed to the replication studies.


Notes from call 2/7/2022:
Test moderators: sample size, p value, new participant data vs new existing data, etc
Original variables to code for across papers not with replication outcomes?
Conversion of effect sizes of concern - can’t do all, concerns about it, etc
To do: map out assumptions and variability across replication/original outcomes


Nuance: replication vs generalizability (we merge this)
Bushel/single (merged or split out?)
Null/positive findings (particularly with bushel)


Possibility (effort driven) is if we should code out ‘non-confirmatory’ effects in replications or to just leave them in the replication reports.


Can we easily capture data about whether a replication was possible (SCORE constraints, ethics, etc)? (communicate how this is influenced by CE as well)
[a]should be 62, check later
[b]I rewrote the next four paragraphs to test out a description of the selection process and procedure that aligns with the Figure drafting happening in: https://docs.google.com/presentation/d/1lDA-0lNrp40QwluCFISVMfSIKOKSzcEc4uEGf7ZoC8M/edit#slide=id.g2a8dff39266_1_29. 


The description is pretty long for being in the introduction, but it might be needed to provide the reader with sufficient context to understand the result reporting. If a shorter version is feasible, some of the details could be moved to the methods. @andrewtyner@cos.io @noah@cos.io
[c]Figure describes the overall flow of claims evaluated, staging, key sample stats, etc). Serves as a combo-style PRISMA + Desc Stats figure
[d]I tried sketching out a starting description around this figure. Assumption of text is that the figure is focused on the sample, not necessarily combining with descriptive stats. Also, to what extent can we effectively highlight the cases of non-independence in such a Figure?
[e]old text: Second, it was possible for two claims from the same paper to be subjected to replication or the same claim to be replicated more than once. For {} papers, more than one claim was subjected to replication. And, {} claims were replicated more than once. Together, the {} replicated claims came from {} papers. This introduces a small but meaningful amount of non-independence in the dataset. To address this, we XXXX. Further detail is available in the methods and supplementary information. [any specific insight or conclusion to make]
[f]Following the lead of the PNAS replication paper, we could include others here: Bayes Factor, Small Telescopes.
[g]If the simple description of the selection processes in the prior paragraph is insufficient. For example, here could focus on summarizing the distribution across fields and years and if there was any obvious bias in the selection of papers from fields at any stage of completing the replication studies.
[h]Draft: https://docs.google.com/spreadsheets/d/1y923FIrsaM079MJhPaUBOaoEmwxywIVfMKtPWDc9Uvo/edit#gid=158131081
[i]Draft: https://docs.google.com/spreadsheets/d/1y923FIrsaM079MJhPaUBOaoEmwxywIVfMKtPWDc9Uvo/edit#gid=1138937747
[j]If the results will have commentary/context setting for each criterion, then something like this could be here. Otherwise, whatever content is of value from this paragraph could be moved to the methods.
[k]Do we want the number out of claims, or the weighted percentage/proportion out of the number of papers? I.e. if we do just by proportion of all claims, we are disproportionately weighting toward papers with multiple claims
[l]may not be necessary if all cases were amenable to assessment on this criterion.
[m]I believe we did it this way. Correct memory?
[n]Possible additional analysis that was in RP:CB: "A weakness of this approach to assessing replication results is that it treats p = 0.05 as a bright-linecriterion between replication success and failure. For example, if an excess of findings fell just abovep = 0.05 it could indicate false negatives are present in the non-statisticallysignificant outcomes oforiginal positive results. p-valuesfor non-statisticallysignificant replication effects were widely distributed(Figure 1—figure supplement 1), and do not statistically differ from the approximately uniform distribution that would be expected if all were true null results whether examining the findings thathad p-valuesfor both original and replication effects (Fisher’s exact test: χ2(118) = 135.7, p = 0.127),or also including the replication effects for which the original effects were based on a representativeimage (χ2(138) = 155.1, p = 0.152). Therefore, we cannot reject the hypothesis that the observed nulleffects come from a population of true negatives."
[o]The assumed conclusion, but may need revision.
[p]Agrees with everything I've seen so far, but yep pending final look
[q]Distributions of effect sizes in orig vs replications, with movement between them. Main figure of paper. Could be multi-plot with up to 2 dims of stratificiation
[r]There could be two tiers of replication criterion outcomes with all of them reported in a summary table, the ones above given emphasis and then any others that we can consider presented here: 95% CI, prediction intervals, Simonsohn's approach, Bayes, ...


Or, we could invest a lot of words and present all of them in full. This likely would not be acceptable for word limits for some of the likely journals however.
[s]This is a relevant reference for this: https://www.tandfonline.com/doi/pdf/10.1080/00031305.2018.1543138
[t]particularly figure 1.
[u]https://docs.google.com/spreadsheets/d/1y923FIrsaM079MJhPaUBOaoEmwxywIVfMKtPWDc9Uvo/edit#gid=2092302912
[v]analyze overall by discipline and by the specific journals included in this project.
[w]If necessary, the prior paragraph may be enough for a closing.
[x]If necessary, the prior paragraph may be enough for a closing.
[y]Tagging myself here; brief about cluster weighted and clustered errors, proportion differences, and raw proportions